#G08# As a Developer, I want to get a Data Package into Node, so that I can start using the data for doing analysis and visualizations.
#G08# As a Researcher, I want to get a Data Package into Julia in seconds, so that I can start using the data for doing analysis and visualizations. 
#G08# As a Publisher, I want to add type information to my data, so that it is more useful to others and can be used better with tools like visualization programs. 
#G08# As a Publisher, I want to be able to provide a visualization of data in the Data Package, so that I can provide my analysis and show my work to users of the data. 
#G08# As a Researcher, I want to be able to save new visualizations, so that I can share them with others or include them in the Data Package. 
#G08# As a ResearcherPublisher, I want to know that my data conforms to its Data Package profile, so that I can feel trust in the validity and usefulness of the data. 
#G08# As a ResearcherPublisher, I want to understand the ways in which my data is invalid, so that I can know how to fix it. 
#G08# As a Researcher, I want to get a Data Package into R in seconds, so that I can start using the data for doing analysis and visualizations. 
#G08# As a Researcher, I want to get a Data Package into Excel in seconds, so that I can start using the data for doing analysis and visualizations. 
#G08# As a Researcher, I want to get a Data Package into SPSS in seconds, so that I can start using the data for doing analysis and visualizations. 
#G08# As a Researcher, I want to get a Data Package into STATA in seconds, so that I can start using the data for doing analysis and visualizations. 
#G08# As a Researcher, I want to be able to translate my EML dataset to a Data Package, so that I can benefit from the wide array of tools available for Data Packages. 
#G08# As a Researcher, I want to get a Data Package into LibreOffice/OpenOffice in seconds, so that I can start using the data for doing analysis and visualizations. 
#G08# As a Developer, I want to get a Data Package into Python in seconds, so that I can start using the data for doing analysis and visualizations. 
#G08# As a Developer, I want a jQuery plugin for Core Data Packages, so that I can use it to apply to form control that uses a core dataset for autocompletion. 
#G08# As a Researcher, I want to get my Excel spreadsheet into a Data Package, so that I can benefit from better tooling and standardization. 
#G08# As a Developer, I want to do exploratory data analysis in R and operationalize that analysis in Python, so that I can use the best tool for the job.
#G08# As a Developer, I want to get a Data Package into Clojure in seconds, so that I can start using the data in doing analysis and visualizations. 
#G08# As a Developer, I want to get a Data Package into Julia in seconds, so that I can start using the data in doing analysis and visualizations. 
#G08# As a Developer, I want to get a Data Package into C++ in seconds, so that I can start using the data in doing analysis and visualizations. 
#G08# As a Machine Learning expert, I would like to package ML datasets as data packages, so that I can easily import them into my ML platform, so that I can start using the data in doing analysis. 
#G08# As a Developer, I want an Elasticsearch integration, so that I can integrate data-packaged data with pipelines that use Elasticsearch. 
#G08# As a Developer, I want an SPSS integration, so that I can integrate data-packaged data with pipelines that use SPSS. 
#G08# As a Developer, I want an EPrints integration, so that I can integrate data-packaged data with pipelines that use EPrints. 
#G08# As a Developer, I want a Mongo integration, so that I can integrate data-packaged data with pipelines that use Mongo. 
#G08# As a Developer, I want a DAT integration, so that I can integrate data-packaged data with pipelines that use DAT. 
#G08# As a ResearcherGovernment Publisher, I want to add general reference data to my narrow dataset, so that my dataset is more useful. 
#G08# As a ResearcherGovernment Publisher, I want to add general country names to my dataset that only contains country codes, so that my dataset is more readable. 
#G08# As a ResearcherGovernment Publisher, I want to add reference data on inflation to my spending dataset, so that the spending lines in my dataset is more understandable. 
#G08# As a ResearcherGovernment Publisher, I want to map lines in my dataset using geographic data in my dataset, so that my dataset is more engaging for non-technical users. 
#G08# As a Researcher, I want to be able to reference a remote-controlled vocabulary for my dataset, so that I can be sure that column of my dataset are valid against a single shard list of terms. 
#G08# As a developer, I want an DSpace integration, so that I can integrate data-packaged data with pipelines that use Dspace. 
#G08# As a Developer, I want Feather integration, so that I can integrate data-packaged data with pipelines that use Feather. 
#G08# As a Developer, I want HDF5 integration, so that I can integrate data-packaged data with pipelines that use HDF5.
#G08# As a Researcher, working with data, I want an Microsoft Power BI integration, so that I can import datasets without downloading them locally. 
#G08# As a ResearcherPublisher, I want an integration with Zenodo, so that when I store my dataset in GitHub, I donâ€™t have to retype descriptive information about my dataset. 
#G08# As a Publisher, I would like an integration with Open Refine, so that I can output cleaned Data Packages. 
#G08# As a ResearcherPublisher, I want to publish Data Packages to CKAN, so that my data is findable, and I can have a data API. 
#G08# As a ResearcherDeveloper, would like the ability import/export from MS-SQL, so that I can use Data Packages in workflows that involve MS-SQL. 
#G08# As a Researcher, working with data in NetCDF, I want NetCDF integration, so that I can store my data in plaintext while still retaining its metadata. 
#G08# As a Researcher, I want an integration with https://data.mendeley.com/, so that I can validate my data upon ingest to the service.
#G08# As a Publisher, I would like an integration with Excel, so that I can output cleaned Data Packages. 
#G08# As a Publisher, I want to store my data quickly and easily online. 
#G08# As a Repository Manager, I want a tool that makes it easy for researchers/ users to add basic metadata to their research data, so that it is more findable and therefore useful. 
#G08# As a ResearcherPublisher, I want validate my data with a minimum of clicks, so that I can feel trust in the validity and usefulness of the data. 
#G08# As a publisher, I want to be able to check that every time I update my data it is still good, so that I can catch errors early and store reliable data. 
#G08# As a DeveloperWrangler, I want to use a command line tool that allows met to validate data, so that I can feel trust in the validity and usefulness of the data quickly and in the context of my command line workflow. 
#G08# As a developer, I want an online service that is connected to my data repository that validates data on update, so that I can delegate data validation to a third party. 
#G08# As a government Publisher, I want to make it easy to prove that our published data is valid, so that I can claim that we are living up to our transparency commitments. 
#G08# As a Civic Tech Activist, I want to make it easy to assess the quality of data stored by the government, so that I can make sure that government is living up to its transparency commitments. 
#G08# As a publisher, I want to embed an interactive preview of my data on my site, so that users can be encouraged that this is the correct data for them. 
#G08# As a publisher, I want to embed a preview button on my site, so that users can preview the data and be encouraged that this is the correct data for them.
#G08# As a Publisher, I want to know how many users have previewed a dataset, so that I know how interest in a dataset relates to its actual download numbers. 
#G08# As a Developer, I want to customize an existing wizard for my specific type of data, so that I can give my users a great user experience.
#G08# As a Publisher, I want to add useful metadata or add in new data columns to make the dataset more useful. 
#G08# As a publisher, I want to package reproducible steps to get a certain data state, so my methodology is transparent and can be rerun by others. 
#G08# As a DeveloperDataWrangler, I want to store my Data Package in GitHub and have it automatically stored into CKAN, so that I get a data API and my dataset is listed in a relevant catalog. 
#G08# As a Researcher, I want a tool that can generate basic statistics about a dataset, so that I can get a quick preview of the data. 
#G08# As a DeveloperPublisher, I want a tool to create an embeddable data summary via iframe, so that I can embed data summaries across sites. 
#G08# As a Researcher, I want an app that generates an OpenRefine reconciliation API endpoint from a Tabular Data Package, so that I can use it to clean messy data. 
#G08# As a Researcher, I want an app that create proxy Data Packages for well know and reliable data, sources, so that I can load high quality data using Data Package tooling. 
#G08# As a RepositoryManagerResearcher, I want an app that acts as a match-making service for packaging data, so that owners are paired with data packagers. 
#G08# As a developer, I would like to create a web socket protocol for Frictionless data tools, so that I can easily use data packages with additional data analysis tools. 
#G08# As a Publisher, I would like a tool to check data availability persistence after publication. 
#G08# As a ResearcherPublisher, I want to specify the funding that contributed to the creation of a given dataset, so that funding agencies can identify the funding, source for a given dataset. 
#G08# As a ResearcherPublisher, I want to add a DOI to a dataset, so that I can cite it in papers stored  with the data. 
#G17# As an app developer, I want to include the code of a dataset type in my app artifact and create a dataset of that type when deploying the app.
#G17# As an app developer, I want to deploy a new version of a dataset type as part of deploying a new version of the app that includes it and I expect that all dataset instances of that type that were created as part of the app deployment start using the new code.
#G17# As an app developer, I want to deploy a new version of a dataset type as part of an app artifact, without affecting other datasets of this type.
#G17# As an app developer, I want to explore a dataset instance of a type that was deployed as part of an app.
#G17# As an app developer, I want to ensure that when I deploy an artifact without creating an app this will not create any dataset types or instances.
#G17# As an app developer, I want to share a dataset type across multiple applications that include the dataset type's code in their artifacts.
#G17# As an app developer, I want to ensure that when I deploy a new version of an app that includes a shared dataset type that all dataset instances created by this app start using the new code but all dataset instances created by other apps remain unchanged.
#G17# As an app developer, I want to ensure that when I deploy a new version of an app that includes an older version of a dataset type deployed by another app and I expect that the dataset instances created by this app use the dataset type code included in this app.
#G17# As an app developer, I want to ensure that when I deploy a new version of an app that includes a different version of a dataset type deployed by another app and this app shares a dataset instance of this type with the other app the deployment will fail with a version conflict error. 
#G17# As an app developer, I want to share a dataset type that I had previously deployed as part of an app.
#G17# As a dataset developer, I want to deploy a dataset type independent from any app and allow apps to create and use dataset instances of that type.
#G17# As a dataset developer, I want to have the option of forcing applications to have the dataset code injected at runtime.
#G17# As a dataset developer, I want to have an archetype that helps me package my dataset type properly.
#G17# As a dataset developer, I want to separate the interface from the implementation of a dataset type.
#G17# As an app developer, I want to only depend on the interface of a dataset type in my app and have the system inject the implementation at runtime.
#G17# As an app developer, I want to write unit tests for an app that depends on the interface of a dataset type.
#G17# As a dataset developer, I want to assign explicit versions to the code of a dataset type.
#G17# As a dataset developer, I want to deploy a new version of a dataset type without affecting the dataset instances of that type.
#G17# As an app developer, I want to create a dataset instance with a specific version of a dataset type.
#G17# As a dataset developer, I want to explore a dataset instance created from a dataset type that was deployed by itself.
#G17# As a dataset developer, I want to delete outdated versions of a dataset type and I expect this to fail if there are any dataset instances with that version of the type.
#G17# As a dataset developer, I want to list all dataset instances that use a dataset type or a specific version of a type.
#G17# As a data scientist, I want to be able to create a dataset instance of an existing dataset type without writing code.
#G17# As a data scientist, I want to be able to upgrade a dataset instance to a new version of its code.
#G17# As a hydrator user, I want to create a pipeline that reads or writes an existing dataset instance.
#G17# As a hydrator user, I want to create a pipeline that reads or writes a new dataset instance and I want to create that dataset instance as part of pipeline creation.
#G17# As a hydrator user, I want to specify an explicit version of the dataset types of the dataset instances created by my pipeline and I expect pipeline creation to fail if that results in incompatible upgrade of an existing dataset instance that is shared with other apps or pipelines.
#G17# As a hydrator user, I want to explore the datasets created by my pipeline.
#G17# As a hydrator user, I want to ensure that all dataset instances created by apps are available as sinks and sources for pipelines.
#G17# As an app developer, I want to ensure that all dataset instances created by Hydrator pipelines are accessible to the app.
#G17# As a plugin developer, I want to include the code for a dataset type in the plugin artifact, so that when a pipeline using this plugin is created a dataset instance of that type is created and it is explorable and available to apps.
#G17# As a plugin developer, I want to use a custom dataset type that was deployed independently or as part of an app inside the plugin. 
#G17# As a plugin developer, I want to upgrade the code of a dataset type used by a dataset instance created by that plugin when I deploy a new version of the plugin and update the pipeline to use that version.
#G17# As a pipeline developer, I want to upgrade a dataset instance to a newer version of the code after the pipeline was created.
#G17# As a dataset developer, I want to have the option of implementing an upgrade step for when a dataset instance is upgraded to a new version of the dataset type.
#G17# As a dataset developer, I want to have a way to reject an upgrade of a dataset instance to a newer version of it type if the upgrade is not compatible.
#G17# As a dataset developer, I want to have the option of implementing a migration procedure that can be run after an upgrade of a dataset instance to a new version of it type. 
#G17# As a developer, I want to take a dataset offline, so that I can perform a long-running maintenance or migration procedure.
#G17# As a dataset developer, I want to implement custom administrative operations such as "compaction" or "rebalance" that are no common to all dataset types.
#G17# As an app developer, I want to perform custom administrative operations on dataset instances from my app and the CLI and REST or the UI.
#G17# As a user, I want to find out what properties are supported by the dataset type what values are allowed and what the defaults are when creating a dataset instance. 
#G17# As a user, I want to specify the schema of a dataset in a uniform way across all dataset types.
#G17# As a user, I want to specify schema as a JSON string.
#G17# As a user, I want to specify schema as a SQL schema string.
#G17# As a user, I want to configure time-to-live in a uniform way across all dataset types.
#G17# As a user, I want to see the properties that were used to configure a dataset instance.
#G17# As a user, I want to find out what properties of a dataset can be updated.
#G17# As a user, I want to update the properties of a dataset instance and I expect this to fail if the new properties are not compatible with a meaningful error message.
#G17# As a user, I want to update a single property of a dataset instance without knowing all other properties. 
#G17# As a user, I want to remove a single property of a dataset instance without knowing all other properties. 
#G17# As a user, I want to trigger a migration process for a dataset if updating its properties requires that.
#G17# As a user, I want to ensure that if reconfiguration of a dataset fails then no changes have taken effect, so that all steps required to reconfigure a dataset must be done as a single atomic action.
#G17# As an app developer, I want to ensure that application creation fails if any of its datasets cannot be created.
#G17# As an app developer, I want to ensure that application redeployment fails if any of its datasets cannot be reconfigured.
#G17# As an app developer, I want to tolerate existing datasets if their properties are different but compatible when creating a dataset as part of app deployment. 
#G17# As a pipeline designer, I want to get a meaningful error message when pipeline creation fails when I use an existing dataset as a sink or source, so that I know that the schema or any other property of the dataset is incompatible with what the pipeline requires. 
#G17# As a user, I want to specify as part of dataset configuration whether it is explorable.
#G17# As a user, I want to specify the explore schema separately.
#G17# As a user, I want to ensure that dataset creation fails if the dataset cannot be enabled for explore.
#G17# As a user, I want to ensure that dataset reconfiguration fails if the corresponding update of the explore table fails.
#G17# As a user, I want to ensure that a dataset operation fails if it fails to make its required changes to explore.
#G17# As a user, I want to ensure that an update of explore never leads to silent loss of data or data available for explore. 
#G17# As a user, I want to enable explore for a dataset that was not configured for explore initially.
#G17# As a user, I want to disable explore for a dataset that was configured for explore initially.
